{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you have a working knowledge of CNNs and have practiced implementing associated techniques in Keras, its time to put all of those skills together. In this lab, you'll work to complete a Kaggle competition on classifying dog breeds.\n",
    "\n",
    "https://www.kaggle.com/c/dog-breed-identification\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Independently design and build a CNN for image classifcation tasks\n",
    "* Compare and apply multiple techniques for tuning a model including data augmentation and adapting pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Load the Data\n",
    "\n",
    "Start by downloading the data locally and loading it into a Pandas DataFrame. Be forewarened that this dataset is fairly large and it is advisable to close other memory intensive applications.\n",
    "\n",
    "The data can be found here:\n",
    "\n",
    "https://www.kaggle.com/c/dog-breed-identification/data\n",
    "\n",
    "We recommend downloading the data into this directory on your local computer. From there, be sure to uncompress the folder and subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No code persay, but download and decompress the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now that you've downloaded the data, its time to prepare it for some model building! You'll notice that the current structure provided is not the same as our lovely preprocessed folders that we've been providing you. Instead, you have one large training folder with images and a csv file with labels associated with each of these file types. \n",
    "\n",
    "Use this to create a directory substructure for a train-validation-test split as we have done previously. Also recall from our previous work that you'll also want to use one-hot encoding as we are now presented with a multi-class problem as opposed to simple binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; open the labels.csv file stored in the zip file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('labels.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/test.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move('test.txt', 'data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(X, train_size = 0.75, val_size = 0.15, test_size = 0.15, random_state = None):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    Xtv, test = train_test_split(X, test_size = test_size, shuffle = True, random_state = random_state)\n",
    "    train, val = train_test_split(Xtv, test_size = val_size, shuffle = True, random_state = random_state)\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "breeds = labels.groupby('breed')\n",
    "train_s, val_s, test_s = 0.70, 0.15, 0.15\n",
    "for breed, filenames in breeds:\n",
    "    # Generate train/val/test\n",
    "    train, val, test = train_val_test(filenames.id)\n",
    "    files_dict = {'train':train,'val':val,'test':test}\n",
    "    for folder,IDs in files_dict.items():\n",
    "        filepath = 'data/'+folder+'/'+breed\n",
    "        os.mkdir(filepath)\n",
    "        for ID in IDs:\n",
    "            src = 'unsorted/'+ID+'.jpg'\n",
    "            dest = filepath+'/'+ID+'.jpg'\n",
    "            shutil.move(src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to create our standard directory structure:\n",
    "* train\n",
    "    * category1\n",
    "    * category2\n",
    "    * category3\n",
    "    ...\n",
    "* val\n",
    "    * category1\n",
    "    * category2\n",
    "    * category3\n",
    "    ...\n",
    "* test \n",
    "    * category1\n",
    "    * category2\n",
    "    * category3\n",
    "    ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7278 images belonging to 120 classes.\n",
      "Found 1352 images belonging to 120 classes.\n",
      "Found 1592 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; transform the image files and then load them into Keras as tensors \n",
    "#(be sure to perform a train-val-test split)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255) \n",
    "train_generator = datagen.flow_from_directory(\n",
    "        'data/train', target_size=(150, 150), \n",
    "        batch_size = 10, \n",
    "        class_mode ='categorical')\n",
    "val_generator = datagen.flow_from_directory(\n",
    "        'data/val', target_size = (150,150),\n",
    "        batch_size = 10,\n",
    "        class_mode = 'categorical')\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        'data/test', target_size = (150,150),\n",
    "        batch_size = 10,\n",
    "        class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Build a Baseline CNN\n",
    "\n",
    "This is an optional step. Adapting a pretrained model will produce better results, but it may be interesting to create a CNN from scratch as a baseline. If you wish to, do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "728/728 [==============================] - 300s 412ms/step - loss: 4.8644 - acc: 0.0092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a55aa1590>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a baseline CNN model\n",
    "from keras import models, layers, optimizers\n",
    "baseline_cnn = models.Sequential()\n",
    "\n",
    "baseline_cnn.add(layers.Conv2D((32), (3,3), activation = 'relu', input_shape = (150,150,3)))\n",
    "baseline_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "baseline_cnn.add(layers.Flatten())\n",
    "baseline_cnn.add(layers.Dense(64, activation='relu'))\n",
    "baseline_cnn.add(layers.Dense(64, activation='relu'))\n",
    "baseline_cnn.add(layers.Dense(120, activation='softmax'))\n",
    "\n",
    "baseline_cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "baseline_cnn.fit_generator(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with the Pretrained Model\n",
    "\n",
    "Now that you've loaded a pretrained model, it's time to adapt that convolutional base and add some fully connected layers on top in order to build a classifier from these feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "15/15 [==============================] - 417s 28s/step - loss: 4.9009 - acc: 0.0333 - val_loss: 4.9482 - val_acc: 0.0081\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 443s 30s/step - loss: 4.9520 - acc: 0.0067 - val_loss: 4.9007 - val_acc: 0.0081\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 400s 27s/step - loss: 4.8006 - acc: 0.0133 - val_loss: 4.8932 - val_acc: 0.0118\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 425s 28s/step - loss: 4.7963 - acc: 0.0000e+00 - val_loss: 4.7706 - val_acc: 0.0104\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 495s 33s/step - loss: 4.7873 - acc: 0.0067 - val_loss: 4.8100 - val_acc: 0.0111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a9b9fee10>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; add fully connected layers on top of the convolutional base\n",
    "from keras.applications import VGG19\n",
    "cnn_VGG19 = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))\n",
    "\n",
    "# set cnn_VGG19 to not trainable\n",
    "cnn_VGG19.trainable = False\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(cnn_VGG19)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(128, activation = 'relu'))\n",
    "model.add(layers.Dense(120, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch = 15, \n",
    "                    epochs = 5, \n",
    "                    validation_data = val_generator,\n",
    "                    validation_steps = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize History\n",
    "\n",
    "Now fit the model and visualize the training and validation accuracy/loss functions over successive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; visualize the training / validation history associated with fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! In this lab, you brought all of your prior deep learning skills together from preprocessing including one-hot encoding, to adapting a pretrained model. There are always ongoing advancements in CNN architectures and best practices, but you have a solid foundation and understanding at this point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
